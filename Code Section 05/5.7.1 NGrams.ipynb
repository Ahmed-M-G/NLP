{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "from future.utils import iteritems\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "\n",
    "def get_sentences():\n",
    "    return brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx():\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print(\"Vocab size:\", i)\n",
    "    return indexed_sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "\n",
    "    word_idx_count = {0: float('inf'),1: float('inf')}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "\n",
    "\n",
    "  # restrict vocab size\n",
    "\n",
    "  # set all the words I want to keep to infinity\n",
    "  # so that they are included when I pick the most\n",
    "  # common words\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة لعمل مصفوفة ضخمة للبايجرام, والتي كلها بقيمة 1 في البداية , ثم يتم ضربها في معامل معين لتقليلها\n",
    "\n",
    "ثم تحديد بداية و نهاية الكلمة \n",
    "\n",
    "ثم اضافة 1 الي كل كلمتين متتاليتين في مكانهما , حيث الصفوف هي الكلمات و الأعمدة هي الكلمات السابقة لها \n",
    "\n",
    "اخيرا  يتم عمل تدريج للبيانات لتقليلها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
    "    # structure of bigram probability matrix will be:\n",
    "    # (last word, current word) --> probability\n",
    "    # we will use add-1 smoothing\n",
    "    # note: we'll always ignore this from the END token\n",
    "    bigram_probs = np.ones((V, V)) * smoothing\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                # beginning word\n",
    "                bigram_probs[start_idx, sentence[i]] += 1\n",
    "            else:\n",
    "                # middle word\n",
    "                bigram_probs[sentence[i-1], sentence[i]] += 1\n",
    "\n",
    "            # if we're at the final word\n",
    "            # we update the bigram for last -> current\n",
    "            # AND current -> END token\n",
    "            if i == len(sentence) - 1:\n",
    "                # final word\n",
    "                bigram_probs[sentence[i], end_idx] += 1\n",
    "\n",
    "    # normalize the counts along the rows to get probabilities\n",
    "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "    return bigram_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قراءة الجمل  , و قاموس الكلمات الي الاندكس من المكتبة التي تم تصميمها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "# note: sentences are already converted to sequences of word indexes\n",
    "# note: you can limit the vocab size if you run out of memory\n",
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
    "# sentences, word2idx = get_sentences_with_word2idx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عدد الجمل و القاموس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences) , len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الجملة الاولي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فحص القاموس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(a,b) in enumerate(word2idx.items()) : \n",
    "    if i ==20 : break\n",
    "    print(a ,'     ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['of']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قاموس عكسي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word= {a : b for b,a in word2idx.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(a,b) in enumerate(idx2word.items()) : \n",
    "    if i ==20 : break\n",
    "    print(a ,'     ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "رؤية بعض الجمل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10) : \n",
    "    phrase = []\n",
    "    for i in sentences[k] : \n",
    "        phrase.append(idx2word[i])\n",
    "    print(' '.join(phrase))\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحديد حجم الكلمات لعمل المصفوفة الضخمة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "V = len(word2idx)\n",
    "print(\"Vocab size:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحديد اندكس البداية و النهاية "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also treat beginning of sentence and end of sentence as bigrams\n",
    "# START -> first word\n",
    "# last word -> END\n",
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "start_idx , end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تطبيق الدالة السابقة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a matrix where:\n",
    "# row = last word\n",
    "# col = current word\n",
    "# value at [row, col] = p(current word | last word)\n",
    "bigram_probs = get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حجمها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دالة لقياس درجة الجملة\n",
    "\n",
    "عبر حساب لوغاريتم كل كلمة علي حدة , ثم حساب المتوسط"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate normalized log prob score\n",
    "# for a sentence\n",
    "def get_score(sentence):\n",
    "    score = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            # beginning word\n",
    "            score += np.log(bigram_probs[start_idx, sentence[i]])\n",
    "        else:\n",
    "            # middle word\n",
    "            score += np.log(bigram_probs[sentence[i-1], sentence[i]])\n",
    "    # final word\n",
    "    score += np.log(bigram_probs[sentence[-1], end_idx])\n",
    "\n",
    "    # normalize the score\n",
    "    return score / (len(sentence) + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دالة لعرض الكلمات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence):\n",
    "    return ' '.join(idx2word[i] for i in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل متغيرات للجمل غير الصحيحة , بحيث تكون غير متناسقة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we sample a fake sentence, we want to ensure not to sample  start token or end token\n",
    "sample_probs = np.ones(V)\n",
    "sample_probs[start_idx] = 0\n",
    "sample_probs[end_idx] = 0\n",
    "sample_probs /= sample_probs.sum()\n",
    "\n",
    "# test our model on real and fake sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الان نقوم بعرض جملة صحيحة و اخري غير صحيحة , وقياس درجة كلا منهما\n",
    "\n",
    "ثم طلب جملة من المستخدم , والتي سنقيس درجتها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # real sentence\n",
    "    real_idx = np.random.choice(len(sentences))\n",
    "    real = sentences[real_idx]\n",
    "\n",
    "    # fake sentence\n",
    "    fake = np.random.choice(V, size=len(real), p=sample_probs)\n",
    "\n",
    "    print(\"REAL:\", get_words(real), \"SCORE:\", get_score(real))\n",
    "    print(\"FAKE:\", get_words(fake), \"SCORE:\", get_score(fake))\n",
    "\n",
    "    # input your own sentence\n",
    "    custom = input(\"Enter your own sentence:\\n\")\n",
    "    custom = custom.lower().split()\n",
    "\n",
    "    # check that all tokens exist in word2idx (otherwise, we can't get score)\n",
    "    bad_sentence = False\n",
    "    for token in custom:\n",
    "        if token not in word2idx:\n",
    "            bad_sentence = True\n",
    "\n",
    "    if bad_sentence:\n",
    "        print(\"Sorry, you entered words that are not in the vocabulary\")\n",
    "    else:\n",
    "        # convert sentence into list of indexes\n",
    "        custom = [word2idx[token] for token in custom]\n",
    "        print(\"SCORE:\", get_score(custom))\n",
    "\n",
    "\n",
    "    cont = input(\"Continue? [Y/n]\")\n",
    "    if cont and cont.lower() in ('N', 'n'):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
